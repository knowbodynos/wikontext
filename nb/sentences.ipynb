{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tools\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "# Data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.spatial.distance as sdist\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Scraping tools\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Viz tools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# NLP tools\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "import spacy\n",
    "snlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Local\n",
    "## Allow local relative imports\n",
    "module_path = os.path.abspath('..')\n",
    "include_path = os.path.join(module_path, 'include')\n",
    "data_path = os.path.join(module_path, 'data')\n",
    "if include_path not in sys.path:\n",
    "    sys.path.append(include_path)\n",
    "from my_nlp import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "sess = requests.Session()\n",
    "wapi_url = \"https://en.wikipedia.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilinks_df = pd.read_csv(data_path + '/clickstream-enwiki-2018-08-bilinks.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bilinks = 100\n",
    "\n",
    "count_df_cols = ['origin_p_count', 'origin_newline_count', 'origin_sent_count', 'n_links_forward',\n",
    "                 'target_p_count', 'target_newline_count', 'target_sent_count', 'n_links_backward']\n",
    "\n",
    "for count_df_col in count_df_cols:\n",
    "    bilinks_df[count_df_col] = 0\n",
    "\n",
    "pos_df_cols = ['origin_p_pos', 'origin_newline_pos', 'origin_sent_pos', 'origin_sent',\n",
    "               'target_p_pos', 'target_newline_pos', 'target_sent_pos', 'target_sent', 'match']\n",
    "\n",
    "if os.path.exists(data_path + '/clickstream-enwiki-2018-08-sentences.tsv'):\n",
    "    sent_df = pd.read_csv(data_path + '/clickstream-enwiki-2018-08-sentences.tsv', sep = '\\t')\n",
    "    start_bilink_ind = len(sent_df.groupby(by = ['origin_title', 'target_title']))\n",
    "    sent_ind = sent_df.shape[0]\n",
    "else:\n",
    "    sent_df = pd.DataFrame(columns = bilinks_df.columns)\n",
    "\n",
    "    for pos_df_col in pos_df_cols:\n",
    "        sent_df[pos_df_col] = 0\n",
    "\n",
    "    sent_df.to_csv(data_path + '/clickstream-enwiki-2018-08-sentences.tsv', sep = '\\t', index = False)\n",
    "    start_bilink_ind = 0\n",
    "    sent_ind = 0\n",
    "\n",
    "for bilink_ind in range(start_bilink_ind, start_bilink_ind + n_bilinks):\n",
    "    sent_df = pd.DataFrame(columns = bilinks_df.columns)\n",
    "\n",
    "    for pos_df_col in pos_df_cols:\n",
    "        sent_df[pos_df_col] = 0\n",
    "\n",
    "    origin_title, target_title = bilinks_df[['origin_title', 'target_title']].iloc[bilink_ind].tolist()\n",
    "\n",
    "    origin_wapi_params = {\n",
    "        'action': \"parse\",\n",
    "        'maxlag': 5,\n",
    "        'page': origin_title,\n",
    "        'prop': \"text\",\n",
    "        'contentformat': \"text/plain\",\n",
    "        'format': \"json\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            origin_data = sess.get(url = wapi_url, params = origin_wapi_params).json()\n",
    "            origin_title_norm = origin_data['parse']['title']\n",
    "            origin_text = origin_data['parse']['text']['*']\n",
    "        except KeyError:\n",
    "            sleep(1)\n",
    "        else:\n",
    "            break\n",
    "    if isinstance(origin_text, bytes):\n",
    "        origin_text = origin_text.decode()\n",
    "    origin_soup = BeautifulSoup(origin_text, 'html5lib')\n",
    "\n",
    "    target_wapi_params = {\n",
    "        'action': \"parse\",\n",
    "        'maxlag': 5,\n",
    "        'page': target_title,\n",
    "        'prop': \"text\",\n",
    "        'format': \"json\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            target_data = sess.get(url = wapi_url, params = target_wapi_params).json()\n",
    "            target_title_norm = target_data['parse']['title']\n",
    "            target_text = target_data['parse']['text']['*']\n",
    "        except KeyError:\n",
    "            sleep(1)\n",
    "        else:\n",
    "            break\n",
    "    if isinstance(target_text, bytes):\n",
    "        target_text = target_text.decode()\n",
    "    target_soup = BeautifulSoup(target_text, 'html5lib')\n",
    "\n",
    "    origin_ref_sents = []\n",
    "    origin_temp_sents = []\n",
    "    p_pos = 0\n",
    "    newline_pos = 0\n",
    "    sent_pos = 0\n",
    "    for p in origin_soup.find_all('p'):\n",
    "        for p_split in p.decode_contents().split('\\n'):\n",
    "            for sent in nltk.sent_tokenize(p_split):\n",
    "                if isinstance(sent, bytes):\n",
    "                    sent = sent.decode()\n",
    "                sent_content = BeautifulSoup(sent, 'html5lib').body.get_text().replace('\\xa0', ' ').encode('ascii', 'ignore').decode()\n",
    "                sent_content = re.sub('\\[.*?\\]', '', sent_content)\n",
    "                sent_content = ' '.join(tok.load(sent_content).word_tokenize(lemmatize = True).word_tokens)\n",
    "                bilinks_df.loc[bilink_ind, 'origin_sent_count'] += 1\n",
    "                sent_pos += 1\n",
    "                if target_title_norm in sent:\n",
    "                    origin_ref_sents.append([p_pos, newline_pos, sent_pos, sent_content])\n",
    "                    bilinks_df.loc[bilink_ind, 'n_links_forward'] += 1\n",
    "                else:\n",
    "                    origin_temp_sents.append([p_pos, newline_pos, sent_pos, sent_content])\n",
    "            bilinks_df.loc[bilink_ind, 'origin_newline_count'] += 1\n",
    "            newline_pos += 1\n",
    "        bilinks_df.loc[bilink_ind, 'origin_p_count'] += 1\n",
    "        p_pos += 1\n",
    "\n",
    "    target_ref_sents = []\n",
    "    target_temp_sents = []\n",
    "    p_pos = 0\n",
    "    newline_pos = 0\n",
    "    sent_pos = 0\n",
    "    for p in target_soup.find_all('p'):\n",
    "        for p_split in p.decode_contents().split('\\n'):\n",
    "            for sent in nltk.sent_tokenize(p_split):\n",
    "                if isinstance(sent, bytes):\n",
    "                    sent = sent.decode()\n",
    "                sent_content = BeautifulSoup(sent, 'html5lib').body.get_text().replace('\\xa0', ' ').encode('ascii', 'ignore').decode()\n",
    "                sent_content = re.sub('\\[.*?\\]', '', sent_content)\n",
    "                sent_content = ' '.join(tok.load(sent_content).word_tokenize(lemmatize = True).word_tokens)\n",
    "                bilinks_df.loc[bilink_ind, 'target_sent_count'] += 1\n",
    "                sent_pos += 1\n",
    "                if origin_title_norm in sent:\n",
    "                    target_ref_sents.append([p_pos, newline_pos, sent_pos, sent_content])\n",
    "                    bilinks_df.loc[bilink_ind, 'n_links_backward'] += 1\n",
    "                else:\n",
    "                    target_temp_sents.append([p_pos, newline_pos, sent_pos, sent_content])\n",
    "            bilinks_df.loc[bilink_ind, 'target_newline_count'] += 1\n",
    "            newline_pos += 1\n",
    "        bilinks_df.loc[bilink_ind, 'target_p_count'] += 1\n",
    "        p_pos += 1\n",
    "\n",
    "    for o in origin_ref_sents:\n",
    "        for t in target_ref_sents:\n",
    "            temp_df = pd.DataFrame(dict(zip(bilinks_df.columns.tolist() + pos_df_cols, bilinks_df.iloc[bilink_ind].tolist() + o + t + [1])), index = [0])\n",
    "            sent_df = sent_df.append(temp_df, sort = False, ignore_index = True).reset_index(drop = True)\n",
    "    \n",
    "    if (len(origin_temp_sents) > len(origin_ref_sents)) and (len(target_temp_sents) > len(target_ref_sents)):\n",
    "        origin_rand_sents = map(lambda i: origin_temp_sents[i], np.random.choice(np.arange(len(origin_temp_sents)), replace = False, size = len(origin_ref_sents)).tolist())\n",
    "        target_rand_sents = map(lambda i: target_temp_sents[i], np.random.choice(np.arange(len(target_temp_sents)), replace = False, size = len(target_ref_sents)).tolist())\n",
    "        for o in origin_rand_sents:\n",
    "            for t in target_rand_sents:\n",
    "                temp_df = pd.DataFrame(dict(zip(bilinks_df.columns.tolist() + pos_df_cols, bilinks_df.iloc[bilink_ind].tolist() + o + t + [0])), index = [0])\n",
    "                sent_df = sent_df.append(temp_df, sort = False, ignore_index = True).reset_index(drop = True)\n",
    "    \n",
    "    sent_df = sent_df.dropna().reset_index(drop = True)\n",
    "    sent_df.to_csv(data_path + '/clickstream-enwiki-2018-08-sentences.tsv', sep = '\\t', mode = 'a', header = False, index = False)\n",
    "\n",
    "    sent_ind += sent_df.shape[0]\n",
    "\n",
    "    if sent_df.shape[0] > 0:\n",
    "        print(\"Title Bilink: {}, Sentence Bilink: {}\".format(bilink_ind, sent_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.read_csv(data_path + '/clickstream-enwiki-2018-08-sentences.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed1 = Word2Vec.load('/Volumes/BlackBoxNew/insight/en_1000_no_stem/en.model')\n",
    "# embed2 = KeyedVectors.load_word2vec_format('/Volumes/BlackBoxNew/insight/enwiki_upos_skipgram_300_5_2017/model.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist1(row):\n",
    "    if isinstance(row['origin_sent'], str) and isinstance(row['target_sent'], str):\n",
    "        u = np.array([embed1.wv[x] for x in row['origin_sent'].split() if x in embed1.wv]).mean(axis = 0)\n",
    "        v = np.array([embed1.wv[x] for x in row['target_sent'].split() if x in embed1.wv]).mean(axis = 0)\n",
    "        return sdist.cosine(u, v), row['match']\n",
    "    else:\n",
    "        return None, row['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist2(row):\n",
    "    if isinstance(row['origin_sent'], str) and isinstance(row['target_sent'], str):\n",
    "        u = np.array([embed2['{}_{}'.format(x.text, x.pos_)] for x in snlp(row['origin_sent']) if '{}_{}'.format(x.text, x.pos_) in embed2]).mean(axis = 0)\n",
    "        v = np.array([embed2['{}_{}'.format(x.text, x.pos_)] for x in snlp(row['target_sent']) if '{}_{}'.format(x.text, x.pos_) in embed2]).mean(axis = 0)\n",
    "        return sdist.cosine(u, v), row['match']\n",
    "    else:\n",
    "        return None, row['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/ipykernel/__main__.py:4: RuntimeWarning: Mean of empty slice.\n",
      "/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.417532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.299429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dist\n",
       "match          \n",
       "0.0    0.417532\n",
       "1.0    0.299429"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1 = sent_df[['origin_sent', 'target_sent', 'match']].apply(cosine_dist1, axis = 1, result_type = 'expand').rename(columns = {0: 'dist', 1: 'match'})\n",
    "test_df1.groupby(by = ['match']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2 = sent_df[['origin_sent', 'target_sent', 'match']].apply(cosine_dist2, axis = 1, result_type = 'expand').rename(columns = {0: 'dist', 1: 'match'})\n",
    "test_df2.groupby(by = ['match']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiCorpus('/Volumes/BlackBoxNew/insight/data/en/enwiki-latest-pages-articles.xml.bz2', \n",
    "                  lemmatize = False, dictionary = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = wiki.get_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin_title = \"Boston\"\n",
    "# target_title = \"New_York_City\"\n",
    "\n",
    "# origin_wapi_params = {\n",
    "#     'action': \"parse\",\n",
    "#     'maxlag': 5,\n",
    "#     'page': origin_title,\n",
    "#     'prop': \"text\",\n",
    "#     'format': \"json\"\n",
    "# }\n",
    "\n",
    "# origin_data = sess.get(url = wapi_url, params = origin_wapi_params).json()\n",
    "# origin_text = origin_data['parse']['text']['*']\n",
    "\n",
    "# target_wapi_params = {\n",
    "#     'action': \"parse\",\n",
    "#     'maxlag': 5,\n",
    "#     'page': target_title,\n",
    "#     'prop': \"text\",\n",
    "#     'format': \"json\"\n",
    "# }\n",
    "\n",
    "# target_data = sess.get(url = wapi_url, params = target_wapi_params).json()\n",
    "# target_text = target_data['parse']['text']['*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def w1(origin_text, target_title):\n",
    "#     origin_soup = BeautifulSoup(origin_text, 'html5lib')\n",
    "    \n",
    "#     context_params = []\n",
    "#     sentence_soups = []\n",
    "#     sent_ind = 0\n",
    "#     for p in origin_soup.find_all('p'):\n",
    "#         scale = 0\n",
    "#         context = False\n",
    "#         for p_split in p.decode_contents().split('\\n'):\n",
    "#             for sent in nltk.sent_tokenize(p_split):\n",
    "#                 sent_soup = BeautifulSoup(sent, 'html5lib').body\n",
    "#                 for a in sent_soup.find_all('a'):\n",
    "#                     if a.attrs['href'] == \"/wiki/\" + target_title:\n",
    "#                         context = True\n",
    "#                         loc = sent_ind\n",
    "#                 sentence_soups.append(sent_soup)\n",
    "#                 sent_ind += 1\n",
    "#                 scale += 1\n",
    "#         if context:\n",
    "#             context_params.append((loc, scale / 2))\n",
    "    \n",
    "#     loc, scale = context_params[0]\n",
    "#     context_weights = norm.pdf(np.arange(sent_ind), loc = loc, scale = scale).reshape((1, -1))\n",
    "#     for loc, scale in context_params[1:]:\n",
    "#         next_weights = norm.pdf(np.arange(sent_ind), loc = loc, scale = scale).reshape((1, -1))\n",
    "#         context_weights = np.concatenate((context_weights, next_weights), axis = 0)\n",
    "    \n",
    "#     return sentence_soups, context_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_soups(sentence_soups, tokenizer = Tokenizer()):\n",
    "#     sentence_htmls = []\n",
    "#     sentence_tokens = []\n",
    "#     for sentence_soup in sentence_soups:\n",
    "#         sentence_htmls.append(sentence_soup.decode_contents())\n",
    "#         sentence_text = re.sub('\\[.*?\\]', '', sentence_soup.get_text())\n",
    "#         word_tokens = tok.load(sentence_text).word_tokenize(lemmatize = True).word_tokens\n",
    "# #         if len(word_tokens) > 0:\n",
    "#         sentence_tokens.append(word_tokens)\n",
    "#     return sentence_htmls, sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def v1(origin_sentence_tokens, target_sentence_tokens):\n",
    "#     max_df = 0.9\n",
    "#     min_df = 1\n",
    "\n",
    "#     max_features = 1000\n",
    "\n",
    "#     min_n_gram = 1\n",
    "#     max_n_gram = 1\n",
    "\n",
    "#     tfidf_vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df,\n",
    "#                                        max_features = max_features,\n",
    "#                                        ngram_range = (min_n_gram, max_n_gram),\n",
    "#                                        stop_words = 'english').fit([' '.join(x) for x in origin_sentence_tokens])\n",
    "    \n",
    "#     origin_sentence_vectors = tfidf_vectorizer.transform([' '.join(x) for x in origin_sentence_tokens]).toarray()\n",
    "#     target_sentence_vectors = tfidf_vectorizer.transform([' '.join(x) for x in target_sentence_tokens]).toarray()\n",
    "    \n",
    "#     return origin_sentence_vectors, target_sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(origin_title, origin_text, target_title, target_text, weight_sentences, vectorize_sentences, metric = 'cosine', tokenizer = Tokenizer()):\n",
    "#     origin_sentence_soups, origin_context_weights = weight_sentences(origin_text, target_title)\n",
    "#     origin_sentence_htmls, origin_sentence_tokens = format_soups(origin_sentence_soups, tokenizer = tok)\n",
    "\n",
    "#     target_sentence_soups, target_context_weights = weight_sentences(target_text, origin_title)\n",
    "#     target_sentence_htmls, target_sentence_tokens = format_soups(target_sentence_soups, tokenizer = tok)\n",
    "\n",
    "#     origin_sentence_vectors, target_sentence_vectors = vectorize_sentences(origin_sentence_tokens, target_sentence_tokens)\n",
    "#     # origin_zero_vecs = origin_sentence_vectors.any(axis = 1)\n",
    "#     # target_zero_vecs = target_sentence_vectors.any(axis = 1)\n",
    "\n",
    "#     # origin_context_weights = origin_context_weights[:, origin_zero_vecs]\n",
    "#     # origin_sentence_vectors = origin_sentence_vectors[origin_zero_vecs, :]\n",
    "#     # target_context_weights = target_context_weights[:, target_zero_vecs]\n",
    "#     # target_sentence_vectors = target_sentence_vectors[target_zero_vecs, :]\n",
    "\n",
    "#     compare = sdist.cdist(origin_sentence_vectors, target_sentence_vectors, metric = metric)\n",
    "\n",
    "#     # origin_origin_context_weights.dot(compare).min(axis = 1)\n",
    "#     # target_context_weights.dot(compare.T).min(axis = 1)\n",
    "\n",
    "#     origin_baseline = compare[origin_context_weights.argmax(axis = 1), 0].tolist()\n",
    "#     target_baseline = compare[0, target_context_weights.argmax(axis = 1)].tolist()\n",
    "\n",
    "#     result = compare[origin_context_weights.argmax(axis = 1), target_context_weights.argmax(axis = 1)].tolist()\n",
    "\n",
    "#     return (origin_baseline + target_baseline), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.656806457662337, 0.9901098242272428, 0.9953238158145499],\n",
       " [0.9746678147737122, 0.7899896999065671])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_data(origin_title, origin_text, target_title, target_text, w1, v1, metric = 'cosine', tokenizer = tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 399 is out of bounds for axis 0 with size 399",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-414-6419d6879420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morigin_context_sentence_inds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mcontext_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin_sentence_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sentence_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cosine'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 399 is out of bounds for axis 0 with size 399"
     ]
    }
   ],
   "source": [
    "# distances = []\n",
    "\n",
    "# titles_request = []\n",
    "# for index, row in df[:100].iterrows():\n",
    "#     titles_request.extend(row[['prev_title', 'curr_title']].tolist())\n",
    "\n",
    "# wapi_params = {\n",
    "#     'action': \"query\",\n",
    "#     'maxlag': 5,\n",
    "#     'prop': \"revisions\",\n",
    "#     'titles': '|'.join(titles_request),\n",
    "#     'rvprop': \"content\",\n",
    "#     'rvslots': \"main\",\n",
    "#     'format': \"json\"\n",
    "# }\n",
    "# data = sess.get(url = wapi_url, params = wapi_params).json()\n",
    "\n",
    "# for j in range(0, 1000, (50 // 2)):\n",
    "#     titles_request = []\n",
    "#     for index, row in df[j:j + (50 // 2)].iterrows():\n",
    "#         titles_request.extend(row[['prev_title', 'curr_title']].tolist())\n",
    "\n",
    "#     wapi_params = {\n",
    "#         'action': \"query\",\n",
    "#         'maxlag': 5,\n",
    "#         'prop': \"revisions\",\n",
    "#         'titles': '|'.join(titles_request),\n",
    "#         'rvprop': \"content\",\n",
    "#         'rvslots': \"main\",\n",
    "#         'format': \"json\"\n",
    "#     }\n",
    "#     data = sess.get(url = wapi_url, params = wapi_params).json()\n",
    "\n",
    "#     for index, row in df[j:j + (50 // 2)].iterrows():\n",
    "#         origin_title, target_title = row[['prev_title', 'curr_title']].tolist()\n",
    "\n",
    "#         norm_origin_title = origin_title\n",
    "#         norm_target_title = target_title\n",
    "#         for title in data['query']['normalized']:\n",
    "#             if title['from'] == origin_title:\n",
    "#                 norm_origin_title = title['to']\n",
    "#             elif title['from'] == target_title:\n",
    "#                 norm_target_title = title['to']\n",
    "#         for page in data['query']['pages'].values():\n",
    "#             if page['title'] == norm_origin_title:\n",
    "#                 origin_wiki = page['revisions'][0]['slots']['main']['*'].replace('\\n', ' ')\n",
    "#             elif page['title'] == norm_target_title:\n",
    "#                 target_wiki = page['revisions'][0]['slots']['main']['*'].replace('\\n', ' ')\n",
    "\n",
    "#         origin_sentence_tokens = []\n",
    "#         origin_context_sentence_inds = []\n",
    "#         i = 0\n",
    "#         for origin_wiki_sentence in nltk.sent_tokenize(origin_wiki):\n",
    "#             origin_content = mwparserfromhell.parse(origin_wiki_sentence).strip_code()\n",
    "#             origin_sentence_text = re.sub('\\[.*?\\]', '', origin_content)\n",
    "#             origin_sentence_tokens_list = tok.load(origin_sentence_text).tokenize(lemmatize = True).sentence_tokens\n",
    "#             if len(origin_sentence_tokens_list) > 0:\n",
    "#                 origin_sentence_tokens.append(origin_sentence_tokens_list[0])\n",
    "#                 if '[[' + norm_target_title in origin_wiki_sentence:\n",
    "#                     origin_context_sentence_inds.append(i)\n",
    "#             i += 1\n",
    "\n",
    "#         target_sentence_tokens = []\n",
    "#         for target_wiki_sentence in nltk.sent_tokenize(target_wiki):\n",
    "#             target_content = mwparserfromhell.parse(target_wiki_sentence).strip_code() \n",
    "#             target_sentence_text = re.sub('\\[.*?\\]', '', target_content)\n",
    "#             target_sentence_tokens_list = tok.load(target_sentence_text).tokenize(lemmatize = True).sentence_tokens\n",
    "#             if len(target_sentence_tokens_list) > 0:\n",
    "#                 target_sentence_tokens.append(target_sentence_tokens_list[0])\n",
    "\n",
    "#         count_vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df,\n",
    "#                                            max_features = max_features,\n",
    "#                                            ngram_range = (min_n_gram, max_n_gram),\n",
    "#                                            stop_words = 'english').fit([' '.join(x) for x in origin_sentence_tokens])\n",
    "\n",
    "#         origin_sentence_vectors = count_vectorizer.transform([' '.join(x) for x in origin_sentence_tokens]).toarray()\n",
    "#         target_sentence_vectors = count_vectorizer.transform([' '.join(x) for x in target_sentence_tokens]).toarray()\n",
    "\n",
    "#         dist = []\n",
    "#         for k in origin_context_sentence_inds:\n",
    "#             context_dist = sdist.cdist(origin_sentence_vectors, target_sentence_vectors, metric = 'cosine')[k]\n",
    "#             if ~np.any(np.isnan(context_dist)):\n",
    "#                 dist.append(context_dist)\n",
    "#         if len(dist) > 0:\n",
    "#             dist = np.array(dist).mean(axis = 0).min()\n",
    "#             distances.append(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 396x306 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with PdfPages(data_path + '/tfidf_cosine_hist.pdf') as pdf:\n",
    "#     fig, ax = plt.subplots()\n",
    "#     fig.set_size_inches(0.5 * 11, 0.5 * 8.5)\n",
    "#     ax.hist(distances, bins = 50)\n",
    "#     ax.grid()\n",
    "#     ax.set_title(\"TF-IDF Cosine Distance Histogram\", fontsize = 20)\n",
    "#     ax.set_xlabel(\"TF-IDF Cosine Distance\")\n",
    "#     ax.set_ylabel(\"Frequency\")\n",
    "    \n",
    "#     pdf.savefig(fig)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + '/links.pickle', 'rb') as handle:\n",
    "#     links_dict = pickle.load(handle)\n",
    "\n",
    "# with open(data_path + '/titles.pickle', 'rb') as handle:\n",
    "#     titles = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wiki_labeled_pair_gen(titles, links_dict, n_titles = 100, maxlag = 5):\n",
    "#     sess = requests.Session()\n",
    "#     wapi_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "#     for origin_inds in grouper(links_dict, n_titles):\n",
    "#         wapi_params = {\n",
    "#             'action': \"query\",\n",
    "#             'maxlag': maxlag,\n",
    "#             'prop': \"revisions\",\n",
    "#             'titles': '|'.join([titles[origin_ind - 1] for origin_ind in origin_inds if not origin_ind is None]),\n",
    "#             'rvprop': \"content\",\n",
    "#             'rvslots': \"main\",\n",
    "#             'format': \"json\"\n",
    "#         }\n",
    "#         origin_data = sess.get(url = wapi_url, params = wapi_params).json()\n",
    "# #         normalized_titles = {}\n",
    "# #         for x in origin_data['query']['normalized']:\n",
    "# #             normalized_titles[x['from']] = x['to']\n",
    "#         origin_pages = iter(origin_data['query']['pages'].values())\n",
    "#         for i in range(len(origin_inds)):\n",
    "#             origin_ind = origin_inds[i]\n",
    "#             if origin_ind is None:\n",
    "#                 break\n",
    "#             origin_sentences = [y for x in next(origin_pages)['revisions'][0]['slots']['main']['*'].split('\\n') for y in nltk.sent_tokenize(x)]\n",
    "#             for target_ind in links_dict[origin_ind]:\n",
    "#                 for origin_sentence in origin_sentences:\n",
    "#                     if (titles[target_ind - 1] + \"]]\" in origin_sentence) or (titles[target_ind - 1].replace('_', ' ') + \"]]\" in origin_sentence):\n",
    "#                         yield (titles[origin_ind - 1], titles[target_ind - 1], origin_sentence, mwparserfromhell.parse(origin_sentence).strip_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = wiki_labeled_pair_gen(titles, links_dict, n_titles = 10, maxlag = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df = 0.9\n",
    "# min_df = 1\n",
    "\n",
    "# max_features = 1000\n",
    "\n",
    "# min_n_gram = 1\n",
    "# max_n_gram = 1\n",
    "\n",
    "# count_vectorizer = CountVectorizer(min_df = min_df, max_df = max_df,\n",
    "#                                    max_features = max_features,\n",
    "#                                    ngram_range = (min_n_gram, max_n_gram),\n",
    "#                                    stop_words = 'english').fit([' '.join(x) for x in sentence_tokens[0]])\n",
    "# a = count_vectorizer.transform([' '.join(x) for x in sentence_tokens[0]]).toarray()\n",
    "# b = count_vectorizer.transform([' '.join(x) for x in sentence_tokens[1]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44,  3, 18, 13, 32, 20, 28,  1, 24, 37,  7, 33, 35, 43, 10,  6, 16,\n",
       "       26, 38, 39,  5, 21,  8,  0, 25, 41, 34, 23, 19, 40, 36, 30, 29, 14,\n",
       "       15, 27, 17, 22, 12,  9, 31,  4,  2, 42, 11])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sdist.cdist(a, b)[0].argsort()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

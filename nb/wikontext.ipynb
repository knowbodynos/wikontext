{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import mwparserfromhell\n",
    "# import wikipedia\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import scipy.spatial.distance as sdist\n",
    "from itertools import zip_longest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Local\n",
    "## Allow local relative imports\n",
    "module_path = os.path.abspath('..')\n",
    "include_path = os.path.join(module_path, 'include')\n",
    "if include_path not in sys.path:\n",
    "    sys.path.append(include_path)\n",
    "\n",
    "from my_nlp import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue = None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(*args, fillvalue = fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_on_doc(tfidf_vectorizer, doc, comp_text, metric = sdist.cosine):\n",
    "    doc_tfidf = tfidf_vectorizer.transform([doc]).toarray()\n",
    "    comp_text_tfidf = tfidf_vectorizer.transform(comp_text).toarray()\n",
    "    scores = []\n",
    "    for i in range(comp_text_tfidf.shape[0]):\n",
    "        if doc_tfidf.dot(comp_text_tfidf[i]) != 0:\n",
    "            scores.append((i, sdist.cosine(doc_tfidf, comp_text_tfidf[i])))\n",
    "    return sorted(scores, key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "with open(module_path + \"/data/titles-sorted.txt\", \"r\") as titles_stream:\n",
    "    for title in titles_stream:\n",
    "        titles.append(title.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dict = {}\n",
    "with open(module_path + \"/data/links-simple-sorted.txt\", \"r\") as links_stream:\n",
    "    for links in links_stream:\n",
    "        origin_str, targets_str = links.rstrip('\\n').split(': ')\n",
    "        origin = int(origin_str)\n",
    "        links_dict[origin] = []\n",
    "        for target in targets_str.split():\n",
    "            target = int(target)\n",
    "            links_dict[origin].append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for origin, targets in list(links_dict.items()):\n",
    "    for target in list(targets):\n",
    "        if not (target in links_dict and origin in links_dict[target]):\n",
    "            links_dict[origin].remove(target)\n",
    "    if len(links_dict[origin]) == 0:\n",
    "        del links_dict[origin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(module_path + '/data/links.pickle', 'wb') as handle:\n",
    "    pickle.dump(links_dict, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(module_path + '/data/titles.pickle', 'wb') as handle:\n",
    "    pickle.dump(titles, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ross/Dropbox/Projects/insight/wikontext/data/links.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4b8e4fed5411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/links.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlinks_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/titles.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ross/Dropbox/Projects/insight/wikontext/data/links.pickle'"
     ]
    }
   ],
   "source": [
    "with open(module_path + '/data/links.pickle', 'rb') as handle:\n",
    "    links_dict = pickle.load(handle)\n",
    "\n",
    "with open(module_path + '/data/titles.pickle', 'rb') as handle:\n",
    "    titles = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_topic = \"Boston\"\n",
    "target_topic = \"New_York_City\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_index = titles.index(origin_topic) + 1\n",
    "target_index = titles.index(target_topic) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f67e15feb7bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://en.wikipedia.org/w/api.php\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m wapi_params = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'action'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'maxlag'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "sess = requests.Session()\n",
    "wapi_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "wapi_params = {\n",
    "    'action': \"query\",\n",
    "    'maxlag': 5,\n",
    "    'prop': \"revisions\",\n",
    "    'titles': titles[:100],\n",
    "    'rvprop': \"content\",\n",
    "    'format': \"json\"\n",
    "}\n",
    "origin_data = sess.get(url = wapi_url, params = wapi_params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_labeled_pair_gen(all_titles, links_dict, n_titles = 100, maxlag = 5):\n",
    "    sess = requests.Session()\n",
    "    wapi_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    for title_inds in grouper(links_dict, n_titles):\n",
    "        titles = []\n",
    "        link_targets = []\n",
    "        for i in title_inds:\n",
    "            if not i is None:\n",
    "                titles.append(all_titles[i - 1])\n",
    "                link_targets.append(links_dict[i])\n",
    "        wapi_params = {\n",
    "            'action': \"query\",\n",
    "            'maxlag': maxlag,\n",
    "            'prop': \"revisions\",\n",
    "            'titles': titles[i:i + n_titles],\n",
    "            'rvprop': \"content\",\n",
    "            'format': \"json\"\n",
    "        }\n",
    "        origin_data = sess.get(url = wapi_url, params = wapi_params).json()\n",
    "        origin_wikitext = list(origin_data['query']['pages'].values())[0]['revisions'][0]['*']\n",
    "        origin_wikisentences = origin_wikitext.split(\". \")\n",
    "        for target in link_dict[i]\n",
    "        origin_target_wikisentences = [x for x in origin_wikisentences if \" [[\" + titles[target - 1].replace(\"_\", \" \") + \"]] \" in x]\n",
    "        origin_target_sentences = [mwparserfromhell.parse(x).strip_code() for x in origin_target_wikisentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(module_path + '/data/links.csv', 'a') as links_stream:\n",
    "    for origin, targets in links_dict.items():\n",
    "        for target in targets:\n",
    "            wapi_params = {\n",
    "                'action': \"query\",\n",
    "                'maxlag': 5,\n",
    "                'prop': \"revisions\",\n",
    "                'titles': titles[origin - 1],\n",
    "                'rvprop': \"content\",\n",
    "                'format': \"json\"\n",
    "            }\n",
    "\n",
    "            origin_data = sess.get(url = wapi_url, params = wapi_params).json()\n",
    "            origin_wikitext = list(origin_data['query']['pages'].values())[0]['revisions'][0]['*']\n",
    "            origin_wikisentences = origin_wikitext.split(\". \")\n",
    "            origin_target_wikisentences = [x for x in origin_wikisentences if \" [[\" + titles[target - 1].replace(\"_\", \" \") + \"]] \" in x]\n",
    "            origin_target_sentences = [mwparserfromhell.parse(x).strip_code() for x in origin_target_wikisentences]\n",
    "\n",
    "            for origin_target_sentence in origin_target_sentences:\n",
    "                print(\"{},{}\".format(origin_target_sentence, titles[origin - 1]), file = links_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['By the mid-18th century, New York City and Philadelphia surpassed Boston in wealth']"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_target_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"path/to/word2vec/en.model\")\n",
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_page = wikipedia.page(origin_topic)\n",
    "target_page = wikipedia.page(target_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "\n",
    "t.load(base_page.content)\n",
    "t.tokenize(lemmatize = True)\n",
    "base_docs = [' '.join(x) for x in t.sentence_tokens]\n",
    "\n",
    "t.load('. '.join([s for s in base_page.content.split(\". \") if target_topic.lower() in s.lower()]))\n",
    "t.tokenize(lemmatize = True)\n",
    "base_sents = [' '.join(x) for x in t.sentence_tokens]\n",
    "\n",
    "t.load(target_page.content)\n",
    "t.tokenize(lemmatize = True)\n",
    "target_docs = [' '.join(x) for x in t.sentence_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.8\n",
    "min_df = 5\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "min_n_gram = 1\n",
    "max_n_gram = 4\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = min_df, max_df = max_df,\n",
    "                                   max_features = max_features,\n",
    "                                   ngram_range = (min_n_gram, max_n_gram),\n",
    "                                   stop_words = 'english').fit(base_docs)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sents_tfidf = tfidf_vectorizer.transform(base_sents).toarray()\n",
    "target_tfidf = tfidf_vectorizer.transform(target_docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by the mid 18th century new york city and philadelphia surpass boston in wealth'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the city and surround area suffer the bulk of the economic damage and largest loss of human life in the aftermath of the september attack when of the terrorist associate with al qaeda pilot american airline flight into the north tower of the world trade center and unite airline flight into the south tower of the world trade center and later destroy them kill civilian firefighter and law enforcement officer who be in the tower and in the surround area',\n",
       " 'hispanic of any race represent of the population while asian constitute the fastest grow segment of the city s population between and the non hispanic white population decline percent the smallest record decline in decade and for the first time since the civil war the number of black decline over a decade',\n",
       " 'some of the natural relief in topography have be even out especially in manhattan the city s total area be square mile km2 include sq mi km2 of land and sq mi km2 of this be water',\n",
       " 'race and ethnicity the city s population in be white non hispanic white black non hispanic black native american and asian',\n",
       " 'flush meadows–corona park in queen with it acre ha make it the city s fourth largest park be the set for the world s fair and the world s fair and be host to the usta billie jean king national tennis center and the annual unite state open tennis championship tournament']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[target_docs[x[0]] for x in score_on_doc(tfidf_vectorizer, base_sents[0], target_docs)[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "\n",
    "t.load(base_page.content)\n",
    "t.tokenize(lemmatize = True)\n",
    "base_docs = [TaggedDocument(x, [i]) for i, x in enumerate(t.sentence_tokens)]\n",
    "\n",
    "t.load('. '.join([s for s in base_page.content.split(\". \") if target_topic.lower() in s.lower()]))\n",
    "t.tokenize(lemmatize = True)\n",
    "base_sents = t.sentence_tokens\n",
    "\n",
    "\n",
    "t.load(target_page.content)\n",
    "t.tokenize(lemmatize = True)\n",
    "target_docs = [TaggedDocument(x, [i]) for i, x in enumerate(t.sentence_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size = 500, window = 2, min_count = 3, workers = 4, seed = 0, epochs = 3)\n",
    "model.build_vocab(target_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99.9 ms, sys: 9.43 ms, total: 109 ms\n",
      "Wall time: 71.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(target_docs, total_examples = model.corpus_count, epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 0.9990196228027344),\n",
       " ('be', 0.9988275766372681),\n",
       " ('of', 0.9987856149673462),\n",
       " ('and', 0.9987653493881226),\n",
       " ('in', 0.9987581372261047),\n",
       " ('a', 0.9984621405601501),\n",
       " ('to', 0.9982845783233643),\n",
       " ('city', 0.998284101486206),\n",
       " ('york', 0.9981852769851685),\n",
       " ('s', 0.998073935508728)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'to',\n",
       " 'in',\n",
       " 'on',\n",
       " 'city',\n",
       " 'area',\n",
       " 's',\n",
       " 'be',\n",
       " 'island',\n",
       " 'york',\n",
       " 'population',\n",
       " 'u',\n",
       " 'the',\n",
       " 'new',\n",
       " 'and',\n",
       " 'largest',\n",
       " 'of',\n",
       " 'manhattan',\n",
       " 'a',\n",
       " 'many',\n",
       " 'state',\n",
       " 'national',\n",
       " 'million',\n",
       " 'center',\n",
       " 'which',\n",
       " 'that',\n",
       " 'with',\n",
       " 'for',\n",
       " 'bronx',\n",
       " 'home',\n",
       " 'number',\n",
       " 'include',\n",
       " 'it',\n",
       " 'by',\n",
       " 'american',\n",
       " 'world',\n",
       " 'all',\n",
       " 'other',\n",
       " 'day',\n",
       " 'park',\n",
       " 'most',\n",
       " 'an',\n",
       " 'at',\n",
       " 'over',\n",
       " 'brooklyn',\n",
       " 'system',\n",
       " 'unite',\n",
       " 'such',\n",
       " 'than',\n",
       " 'long',\n",
       " 'also',\n",
       " 'public',\n",
       " 'street',\n",
       " 'asian',\n",
       " 'have',\n",
       " 'hispanic',\n",
       " 'into',\n",
       " 'borough',\n",
       " 'one',\n",
       " 'lower',\n",
       " 'make',\n",
       " 'staten',\n",
       " 'year',\n",
       " 'industry',\n",
       " 'central',\n",
       " 'metropolitan',\n",
       " 'major',\n",
       " 'square',\n",
       " 'parade',\n",
       " 'acre',\n",
       " 'county',\n",
       " 'immigrant',\n",
       " 'become',\n",
       " 'technology',\n",
       " 'build',\n",
       " 'dutch',\n",
       " 'financial',\n",
       " 'medium',\n",
       " 'billion',\n",
       " 'mile',\n",
       " 'black',\n",
       " 'queen',\n",
       " 'white',\n",
       " 'or',\n",
       " 'first',\n",
       " 'between',\n",
       " 'river',\n",
       " 'economic',\n",
       " 'bridge',\n",
       " 'museum',\n",
       " 'record',\n",
       " 'after',\n",
       " 'while',\n",
       " 'more',\n",
       " 'would',\n",
       " 'group',\n",
       " 'global',\n",
       " 'several',\n",
       " 'approximately',\n",
       " 'high',\n",
       " 'house',\n",
       " 'any',\n",
       " 'time',\n",
       " 'receive',\n",
       " 'increase',\n",
       " 'district',\n",
       " 'annual',\n",
       " 'serve',\n",
       " 'jersey',\n",
       " 'north',\n",
       " 'wall',\n",
       " 'university',\n",
       " 'life',\n",
       " 'grow',\n",
       " 'up',\n",
       " 'neighborhood',\n",
       " 'headquarter',\n",
       " 'airport',\n",
       " 'trade',\n",
       " 'about',\n",
       " 'during',\n",
       " 'international',\n",
       " '°c',\n",
       " 'historic',\n",
       " 'capital',\n",
       " 'km2',\n",
       " 'america',\n",
       " 'second',\n",
       " 'service',\n",
       " 'non',\n",
       " 'foot',\n",
       " 'both',\n",
       " 'important',\n",
       " 'history',\n",
       " 'per',\n",
       " 'highest',\n",
       " 'density',\n",
       " 'outside',\n",
       " 'each',\n",
       " 'water',\n",
       " 'company',\n",
       " 'lead',\n",
       " 'base',\n",
       " 'people',\n",
       " 'top',\n",
       " 'court',\n",
       " 'total',\n",
       " 'since',\n",
       " 'continue',\n",
       " 'within',\n",
       " 'gay',\n",
       " 'resident',\n",
       " 'hudson',\n",
       " 'represent',\n",
       " 'investment',\n",
       " 'film',\n",
       " 'art',\n",
       " 'war',\n",
       " 'sector',\n",
       " 'large',\n",
       " 'who',\n",
       " 'estimate',\n",
       " 'development',\n",
       " 'community',\n",
       " 'among',\n",
       " 'avenue',\n",
       " 'region',\n",
       " 'land',\n",
       " 'call',\n",
       " 'greenwich',\n",
       " 'production',\n",
       " 'busiest',\n",
       " 'their',\n",
       " 'liberty',\n",
       " 'under',\n",
       " 'school',\n",
       " 'market',\n",
       " 'three',\n",
       " 'south',\n",
       " 'this',\n",
       " 'use',\n",
       " 'bear',\n",
       " 'well',\n",
       " '°f',\n",
       " 'venture',\n",
       " 'rank',\n",
       " 'transit',\n",
       " 'king',\n",
       " 'urban',\n",
       " 'pride',\n",
       " 'through',\n",
       " 'west',\n",
       " 'daily',\n",
       " 'ten',\n",
       " 'five',\n",
       " 'greater',\n",
       " 'fort',\n",
       " 'begin',\n",
       " 'construction',\n",
       " 'theater',\n",
       " 'play',\n",
       " 'india',\n",
       " 'private',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'four',\n",
       " 'know',\n",
       " 'east',\n",
       " 'fund',\n",
       " 'june',\n",
       " 'advertise',\n",
       " 'broadway',\n",
       " 'office',\n",
       " 'african',\n",
       " 'irish',\n",
       " 'indian',\n",
       " 'british',\n",
       " 'village',\n",
       " 'foreign',\n",
       " 'significant',\n",
       " 'announce',\n",
       " 'half',\n",
       " 'feature',\n",
       " 'part',\n",
       " 'live',\n",
       " 'los',\n",
       " 'only',\n",
       " 'league',\n",
       " 'july',\n",
       " 'stonewall',\n",
       " 'telecommunication',\n",
       " '–',\n",
       " 'independent',\n",
       " 'landmark',\n",
       " 'whose',\n",
       " 'western',\n",
       " 'free',\n",
       " 'tunnel',\n",
       " 'station',\n",
       " 'around',\n",
       " 'percent',\n",
       " 'cornell',\n",
       " 'european',\n",
       " 'host',\n",
       " 'general',\n",
       " 'nearly',\n",
       " 'atlantic',\n",
       " 'early',\n",
       " 'alley',\n",
       " 'overall',\n",
       " 'volume',\n",
       " 'chinese',\n",
       " 'skyscraper',\n",
       " 'elli',\n",
       " 'biotechnology',\n",
       " 'team',\n",
       " 'nation',\n",
       " 'harbor',\n",
       " 'military',\n",
       " 'science',\n",
       " 'angeles',\n",
       " 'tourism',\n",
       " 'event',\n",
       " 'ferry',\n",
       " 'college',\n",
       " 'commercial',\n",
       " 'zoo',\n",
       " 'end',\n",
       " 'example',\n",
       " 'rate',\n",
       " 'transportation',\n",
       " 'corporation',\n",
       " 'promote',\n",
       " 'decline',\n",
       " 'silicon',\n",
       " 'cultural',\n",
       " 'm2',\n",
       " 'bank',\n",
       " 'madison',\n",
       " 'two',\n",
       " 'range',\n",
       " 'ice',\n",
       " 'along',\n",
       " 'jewish',\n",
       " 'stock',\n",
       " 'they',\n",
       " 'name',\n",
       " 'connect',\n",
       " 'combine',\n",
       " 'battle',\n",
       " 'not',\n",
       " 'populous',\n",
       " 'establish',\n",
       " 'startup',\n",
       " 'form',\n",
       " 'gateway',\n",
       " 'tourist',\n",
       " 'follow',\n",
       " 'series',\n",
       " 'participant',\n",
       " 'commuter',\n",
       " 'legal',\n",
       " 'site',\n",
       " 'annually',\n",
       " 'generate',\n",
       " 'there',\n",
       " 'china',\n",
       " 'midtown',\n",
       " 'diverse',\n",
       " 'flush',\n",
       " 'operate',\n",
       " 'mayor',\n",
       " 'crime',\n",
       " 'research',\n",
       " 'design',\n",
       " 'complete',\n",
       " 'often',\n",
       " 'tower',\n",
       " 'winter',\n",
       " 'washington',\n",
       " 'ethnic',\n",
       " 'monument',\n",
       " 'fifth',\n",
       " 'census',\n",
       " 'm',\n",
       " 'slave',\n",
       " 'side',\n",
       " 'garden',\n",
       " 'fashion',\n",
       " 'hub',\n",
       " 'institution',\n",
       " 'insurance',\n",
       " 'great',\n",
       " 'populate',\n",
       " 'korean',\n",
       " 'subway',\n",
       " 'small',\n",
       " 'fire',\n",
       " 'property',\n",
       " 'source',\n",
       " 'third',\n",
       " 'then',\n",
       " 'refer',\n",
       " 'civil',\n",
       " 'hour',\n",
       " 'rail',\n",
       " 'garment',\n",
       " 'zone',\n",
       " 'outer',\n",
       " 'upon',\n",
       " 'country',\n",
       " 'native',\n",
       " 'memorial',\n",
       " 'style',\n",
       " 'place',\n",
       " 'october',\n",
       " 'segment',\n",
       " 'alone',\n",
       " 'expensive',\n",
       " 'police',\n",
       " 'movement',\n",
       " 'densely',\n",
       " 'take',\n",
       " 'bay',\n",
       " 'exchange',\n",
       " 'individual',\n",
       " 'john',\n",
       " 'israel',\n",
       " 'business',\n",
       " 'extensive',\n",
       " 'respectively',\n",
       " 'tennis',\n",
       " 'temperature',\n",
       " 'yankee',\n",
       " 'statue',\n",
       " 'network',\n",
       " 'battery',\n",
       " 'contain',\n",
       " 'job',\n",
       " 'passenger',\n",
       " 'amsterdam',\n",
       " 'meadow',\n",
       " 'separate',\n",
       " 'hold',\n",
       " 'locate',\n",
       " 'constitute',\n",
       " 'work',\n",
       " 'income',\n",
       " 'port',\n",
       " 'march',\n",
       " 'upper',\n",
       " 'expand',\n",
       " 'local',\n",
       " 'roosevelt',\n",
       " 'average',\n",
       " 'immigration',\n",
       " 'television',\n",
       " 'august',\n",
       " 'choice',\n",
       " 'decade',\n",
       " 'estate',\n",
       " 'leader',\n",
       " 'throughout',\n",
       " 'like',\n",
       " 'yorkers',\n",
       " 'institute',\n",
       " 'economy',\n",
       " 'accord',\n",
       " 'game',\n",
       " 'ever',\n",
       " 'stadium',\n",
       " 'describe',\n",
       " 'account',\n",
       " 'although',\n",
       " 'grind',\n",
       " 'contribution',\n",
       " 'modern',\n",
       " 'corona',\n",
       " 'these',\n",
       " 'his',\n",
       " 'field',\n",
       " 'recreation',\n",
       " 'chocolate',\n",
       " 'come',\n",
       " 'thousand',\n",
       " 'medical',\n",
       " 'england',\n",
       " 'rapid',\n",
       " 'hall',\n",
       " 'impact',\n",
       " 'riot',\n",
       " 'nearby',\n",
       " 'department',\n",
       " 'purchase',\n",
       " 'child',\n",
       " 'plan',\n",
       " 'empire',\n",
       " 'dominican',\n",
       " 'century',\n",
       " 'employ',\n",
       " 'others',\n",
       " 'hundred',\n",
       " 'tournament',\n",
       " 'win',\n",
       " 'descent',\n",
       " 'fourth',\n",
       " 'support',\n",
       " 'jamaica',\n",
       " 'law',\n",
       " 'food',\n",
       " 'hamilton',\n",
       " 'period',\n",
       " 'september',\n",
       " 'encompass',\n",
       " 'republic',\n",
       " 'premier',\n",
       " 'single',\n",
       " 'ha',\n",
       " 'environmental',\n",
       " 'what',\n",
       " 'sport',\n",
       " 'governor',\n",
       " 'entertainment',\n",
       " 'st',\n",
       " 'bloomberg',\n",
       " 'transgender',\n",
       " 'member',\n",
       " 'some',\n",
       " 'german',\n",
       " 'northern',\n",
       " 'primarily',\n",
       " 'show',\n",
       " 'least',\n",
       " 'attempt',\n",
       " 'mean',\n",
       " 'exceed',\n",
       " 'federal',\n",
       " 'higher',\n",
       " 'distinctive',\n",
       " 'escape',\n",
       " 'revolution',\n",
       " 'worker',\n",
       " 'majority',\n",
       " 'political',\n",
       " 'bring',\n",
       " 'until',\n",
       " 'humid',\n",
       " 'provide',\n",
       " 'freedom',\n",
       " 'he',\n",
       " 'nassau',\n",
       " 'francisco',\n",
       " 'but',\n",
       " 'southern',\n",
       " 'baseball',\n",
       " 'i',\n",
       " 'join',\n",
       " 'association',\n",
       " 'ii',\n",
       " 'lie',\n",
       " 'destination',\n",
       " 'era',\n",
       " 'bus',\n",
       " 'green',\n",
       " 'ethnically',\n",
       " 'help',\n",
       " 'kill',\n",
       " 'visitor',\n",
       " 'employment',\n",
       " 'education',\n",
       " 'produce',\n",
       " 'license',\n",
       " 'municipal',\n",
       " 'line',\n",
       " 'rockefeller',\n",
       " 'fight',\n",
       " 'jam',\n",
       " 'role',\n",
       " 'climate',\n",
       " 'location',\n",
       " 'present',\n",
       " 'eastern',\n",
       " 'surround',\n",
       " 'allow',\n",
       " 'grant',\n",
       " 'professional',\n",
       " 'natural',\n",
       " 'real',\n",
       " 'main',\n",
       " 'gallon',\n",
       " 'own',\n",
       " 'program',\n",
       " 'identify',\n",
       " 'human',\n",
       " 'government',\n",
       " 'later',\n",
       " 'burial',\n",
       " 'out',\n",
       " 'settlement',\n",
       " 'chicago',\n",
       " 'contribute',\n",
       " 'near',\n",
       " 'point',\n",
       " 'six',\n",
       " 'skyline',\n",
       " 'championship',\n",
       " 'core',\n",
       " 'geographically',\n",
       " 'reflect',\n",
       " 'protect',\n",
       " 'revenue',\n",
       " 'rule',\n",
       " 'when',\n",
       " 'health',\n",
       " 'right',\n",
       " 'force',\n",
       " 'reach',\n",
       " 'another',\n",
       " 'macy',\n",
       " 'comprise',\n",
       " 'list',\n",
       " 'westchester',\n",
       " 'tip',\n",
       " 'resurgence',\n",
       " 'project',\n",
       " 'set',\n",
       " 'courthouse',\n",
       " 'km',\n",
       " 'attract',\n",
       " 'run',\n",
       " 'italian',\n",
       " 'create',\n",
       " 'netherland',\n",
       " 'facility',\n",
       " 'council',\n",
       " '1990s',\n",
       " 'william',\n",
       " 'longest',\n",
       " 'graduate',\n",
       " 'asia',\n",
       " 'supply',\n",
       " 'english',\n",
       " 'found',\n",
       " 'agency',\n",
       " 'last',\n",
       " 'charles',\n",
       " 'terminal',\n",
       " 'train',\n",
       " 'vehicle',\n",
       " 'share',\n",
       " 'no',\n",
       " 'san',\n",
       " 'suburban',\n",
       " 'broad',\n",
       " 'basketball',\n",
       " 'social',\n",
       " 'compete',\n",
       " 'former',\n",
       " 'colonial',\n",
       " 'citi',\n",
       " 'author',\n",
       " 'light',\n",
       " 'way',\n",
       " 'whom',\n",
       " 'conference',\n",
       " 'original',\n",
       " 'link',\n",
       " 'installation',\n",
       " 'ship']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sent(model, s):\n",
    "    inferred_vec = model.infer_vector(s)\n",
    "    sims = model.wv.most_similar([inferred_vec], topn = len(model.docvecs))\n",
    "[''.join(x[0]) for x in sims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '-0.0015622723' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-2315a3954aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/insight/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '-0.0015622723' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.similarity(model.infer_vector(base_sents[0]), model.infer_vector(target_docs[0].words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by',\n",
       " 'the',\n",
       " 'mid',\n",
       " 'century',\n",
       " 'new',\n",
       " 'york',\n",
       " 'city',\n",
       " 'and',\n",
       " 'philadelphia',\n",
       " 'surpass',\n",
       " 'in']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in base_sents[0] if x in model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
